{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's load our mnist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's train our simple neural network to predict handwritten digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28,28)),\n",
    "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's evaluate our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our model does pretty well and achieves areound **98%** accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's now extract all the images of the digit 2 that we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train = np.where(y_train == 2)\n",
    "x_train_two = x_train[idx_train]\n",
    "y_train_two = y_train[idx_train]\n",
    "num_samples = x_train_two.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's visualize a few random examples of the digit two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=3)\n",
    "fig.tight_layout()\n",
    "for col in ax:\n",
    "    ix = np.random.choice(num_samples)\n",
    "    col.set_title('Label is {label}'.format(label=y_train_two[ix]))\n",
    "    col.imshow(x_train_two[ix], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to find a way to make our original network predict that these images of the digit \"2\" as \"6\". To do that we have to someohow tweak the pixels in the images of \"2\", but only so much so that they still look like the digit \"2\" to the human eye. So we can sum this up with two constraints:\n",
    "\n",
    "* We want to maximize the probability output of our original neural network that the image is the digit 6\n",
    "* We want to minimize how much we change the images so that they don't change too much to the human eye. \n",
    "\n",
    "We'll use the idea from the following blog post: https://medium.com/@ageitgey/machine-learning-is-fun-part-8-how-to-intentionally-trick-neural-networks-b55da32b7196. We'll pick a sample image of two, pass it through our network and use a cost function to calculate the loss given that the image should be predicted as \"6\". We'll then calculate the gradients and tweak our image towards \"6\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's define a function that can give us the gradient and loss based on an image w.r.t to our preferred prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the next two blocks is from this blog here: [link](https://medium.com/@ageitgey/machine-learning-is-fun-part-8-how-to-intentionally-trick-neural-networks-b55da32b7196)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label we want to maximize\n",
    "correct_label = 6\n",
    "\n",
    "#this is a function that grabs the probability from the model of an input being a six\n",
    "prob_func = model.layers[-1].output[0, correct_label] \n",
    "\n",
    "#the gradient w.r.t the input (not the weights of the model) to maximize the prob of our prob_func\n",
    "gradients_func = tf.keras.backend.gradients(prob_func, model.layers[0].input)[0] \n",
    "\n",
    "#function that accepts an image and returns the gradient and prob of image being a six after a forward pass through the network\n",
    "grab_cost_and_gradients_from_model = tf.keras.backend.function([model.layers[0].input, tf.keras.backend.learning_phase()], [prob_func, gradients_func])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = 0.0 #probability of the image being a six\n",
    "learning_rate = 0.1\n",
    "hacked_image = np.copy(x_train_two[0]) #the image that we will tweak to fool the neural net\n",
    "hacked_image = np.expand_dims(hacked_image, axis=0) #add an extra dimension because the model expects a batch\n",
    "\n",
    "# In a loop, keep adjusting the hacked image slightly so that it tricks the model more and more\n",
    "# until it gets to at least 80% confidence\n",
    "iteration = 0\n",
    "while prob < 0.8:\n",
    "    \n",
    "    # Check how close the image is to our target class and grab the gradients we\n",
    "    # can use to push it one more step in that direction.\n",
    "    prob, gradients = grab_cost_and_gradients_from_model([hacked_image, 0])\n",
    "\n",
    "    # Move the hacked image one step further towards fooling the model\n",
    "    hacked_image += gradients * learning_rate\n",
    "    iteration += 1\n",
    "    \n",
    "    #minimize output in notebook\n",
    "    if iteration % 10000 == 0:   \n",
    "        print(\"Model's predicted likelihood that the image is a six: {:.8}%\".format(prob * 100))\n",
    "print(\"Model's predicted likelihood that the image is a six: {:.8}%\".format(prob * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's plot our adversarial image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=3)\n",
    "fig.tight_layout()\n",
    "images = [x_train_two[0], abs(x_train_two[0]-hacked_image[0]), hacked_image[0]]\n",
    "titles = [\"Original image\", \"Delta\", \"Adversarial image\"]\n",
    "for i, col in enumerate(ax):\n",
    "    col.set_title(label=titles[i])\n",
    "    col.imshow(images[i], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad at all, however we can see that the image looks very noisy, to remedy that we to constrain each pixel to not change too much from its original value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's add a constraint that pixels can't change too much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pixel constraint idea and code is also from [here](https://medium.com/@ageitgey/machine-learning-is-fun-part-8-how-to-intentionally-trick-neural-networks-b55da32b7196)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = 0.0\n",
    "learning_rate = 1\n",
    "\n",
    "#pixel can at most change by 0.1 from it's original value\n",
    "max_change = x_train_two[0]+0.1 \n",
    "min_change = x_train_two[0]-0.1 \n",
    "hacked_image = np.copy(x_train_two[0])\n",
    "hacked_image = np.expand_dims(hacked_image, axis=0)\n",
    "\n",
    "# In a loop, keep adjusting the hacked image slightly so that it tricks the model more and more\n",
    "# until it gets to at least 80% confidence\n",
    "iteration = 0\n",
    "while prob < 0.8:\n",
    "    \n",
    "    # Check how close the image is to our target class and grab the gradients we\n",
    "    # can use to push it one more step in that direction.\n",
    "    prob, gradients = grab_cost_and_gradients_from_model([hacked_image, 0])\n",
    "\n",
    "    # Move the hacked image one step further towards fooling the model\n",
    "    hacked_image += gradients * learning_rate\n",
    "    hacked_image = np.clip(hacked_image, min_change, max_change)\n",
    "    iteration += 1\n",
    "    \n",
    "    #minimize output in notebook\n",
    "    if iteration % 10000 == 0:   \n",
    "        print(\"Model's predicted likelihood that the image is a six: {:.8}%\".format(prob * 100))\n",
    "print(\"Model's predicted likelihood that the image is a six: {:.8}%\".format(prob * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's plot the adverserial image with the pixel constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=3)\n",
    "fig.tight_layout()\n",
    "images = [x_train_two[0], abs(x_train_two[0]-hacked_image[0]), hacked_image[0]]\n",
    "titles = [\"Original image\", \"Delta\", \"Adversarial image\"]\n",
    "for i, col in enumerate(ax):\n",
    "    col.set_title(label=titles[i])\n",
    "    col.imshow(images[i], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this is better than before, our delta is mostly white so not much has been added to the image. The adverserial image also has a consistently blacker background than the previous one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's now do this for 10 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = 0.0 #probability of image being a six\n",
    "learning_rate = 0.1\n",
    "\n",
    "#set up original images and copies that we will tweak\n",
    "num_images = 10\n",
    "images_idx = np.arange(0,num_images)\n",
    "original_images = x_train_two[images_idx]\n",
    "hacked_images = np.copy(original_images)\n",
    "\n",
    "#let's use a momentum gradient descent since training was pretty slow using the gradient unmodified\n",
    "velocity = np.zeros((1,28,28), dtype=np.float64)\n",
    "momentum = 1.1\n",
    "\n",
    "for i in range(0,num_images):\n",
    "    \n",
    "    hacked_image = np.copy(hacked_images[i])\n",
    "    max_change = np.copy(original_images[i]+0.1)\n",
    "    min_change = np.copy(original_images[i]-0.1)\n",
    "    hacked_image = np.expand_dims(hacked_image, axis=0)\n",
    "    \n",
    "    # In a loop, keep adjusting the hacked image slightly so that it tricks the model more and more\n",
    "    # until it gets to at least 80% confidence\n",
    "    iteration = 0\n",
    "    while prob < 0.8:\n",
    "\n",
    "        # Check how close the image is to our target class and grab the gradients we\n",
    "        # can use to push it one more step in that direction.\n",
    "        prob, gradients = grab_cost_and_gradients_from_model([hacked_image, 0])\n",
    "        # Move the hacked image one step further towards fooling the model\n",
    "        velocity = momentum*velocity + gradients * learning_rate\n",
    "        hacked_image += velocity\n",
    "        hacked_image = np.clip(hacked_image, min_change, max_change)\n",
    "        iteration += 1\n",
    "\n",
    "        #minimize output in notebook\n",
    "        if iteration % 10000 == 0:   \n",
    "            print(\"Model's predicted likelihood that the image is a six: {:.8}%\".format(prob * 100))\n",
    "        \n",
    "        #Don't do more than 1M iterations\n",
    "        if iteration == 10000:\n",
    "            break\n",
    "    print(\"Done training with a predicted likelihood that the image is  a six: {:.8}%\".format(prob * 100))\n",
    "    hacked_images[i] = hacked_image[0]\n",
    "    velocity = np.zeros((1,28,28), dtype=np.float64)\n",
    "    prob = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's plot our 10 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=10, ncols=3, figsize=(10, 20))\n",
    "plt.subplots_adjust(top = 2, bottom=1, hspace=0.5, wspace=0.4)\n",
    "titles = [\"Original image\", \"Delta\", \"Adversarial image\"]\n",
    "plot_vec = []\n",
    "for i in range(num_images):\n",
    "    plot_vec.append((original_images[i], original_images[i]-hacked_images[i], hacked_images[i]))\n",
    "j = 0\n",
    "for i, row in enumerate(ax):\n",
    "    for j, col in enumerate(row):\n",
    "        col.set_title(label=titles[j])\n",
    "        col.imshow(plot_vec[i][j], cmap='gray', aspect='equal')\n",
    "plt.savefig('adversarial_examples.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
